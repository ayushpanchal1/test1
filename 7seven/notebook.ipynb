{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from collections import Counter, defaultdict\n",
        "import math\n",
        "\n",
        "# Simple tokenizer function\n",
        "def simple_tokenize(text):\n",
        "    return re.findall(r'\\b\\w+\\b', text.lower())  # Extract words ignoring punctuation\n",
        "\n",
        "# Sample corpus\n",
        "corpus = \"\"\"Natural language processing enables computers to understand and generate human language.\n",
        "It involves various techniques like tokenization, parsing, and machine learning.\"\"\"\n",
        "\n",
        "# Tokenize the corpus\n",
        "tokens = simple_tokenize(corpus)\n",
        "\n",
        "# Generate bigrams\n",
        "bigram_list = [(tokens[i], tokens[i+1]) for i in range(len(tokens)-1)]\n",
        "\n",
        "# Count unigrams and bigrams\n",
        "unigram_counts = Counter(tokens)\n",
        "bigram_counts = Counter(bigram_list)"
      ],
      "metadata": {
        "id": "YDmt4AzLKiMf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T0CQmsNXIfb6",
        "outputId": "dbc82d12-8007-4f9d-f745-511d1794516e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Probability of sentence with Add-One Smoothing: 0.00000238095238095238\n"
          ]
        }
      ],
      "source": [
        "# Vocabulary size (unique words in corpus)\n",
        "V = len(set(tokens))\n",
        "\n",
        "# Calculate bigram probabilities with Add-One Smoothing\n",
        "bigram_probabilities = defaultdict(float)\n",
        "for (w1, w2), count in bigram_counts.items():\n",
        "    bigram_probabilities[(w1, w2)] = (count + 1) / (unigram_counts[w1] + V)  # Add-One Smoothing Formula\n",
        "\n",
        "# Function to calculate probability of a sentence with Add-One Smoothing\n",
        "def sentence_probability(sentence):\n",
        "    words = simple_tokenize(sentence)\n",
        "    bigrams_in_sentence = [(words[i], words[i+1]) for i in range(len(words)-1)]\n",
        "\n",
        "    probability = 1.0\n",
        "    for bigram in bigrams_in_sentence:\n",
        "        w1, w2 = bigram\n",
        "        prob = (bigram_counts.get(bigram, 0) + 1) / (unigram_counts.get(w1, 0) + V)  # Add-One Smoothing\n",
        "        probability *= prob\n",
        "\n",
        "    return probability\n",
        "\n",
        "# Example sentence\n",
        "sentence = \"Natural language processing involves machine learning\"\n",
        "probability = sentence_probability(sentence)\n",
        "\n",
        "print(f\"Probability of sentence with Add-One Smoothing: {probability:.20f}\")\n"
      ]
    }
  ]
}